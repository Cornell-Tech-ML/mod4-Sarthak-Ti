{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[12., 17., 12., 11., 20., 12.]]], grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 1, 6]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can test to see what a proper conv is. In the definition he wants, append 0s at the end, and then do the convolution. Let's manually verify with an example\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "I = torch.tensor([1, 2, 4, 1, 1, 6, 0, 0], dtype=torch.float).view(1, 1, 8)\n",
    "\n",
    "m = nn.Conv1d(1, 1, 3, bias=False, padding=0)\n",
    "m.weight.data = torch.tensor([[[2, 3, 1]]], dtype=torch.float)\n",
    "\n",
    "m(I), m(I).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = m(I)\n",
    "a.backward(torch.arange(6).view(1, 1, 6).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[47., 33., 21.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 4 * 2 + 3 + 4 + 6 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 + 2 + 3 + 6 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[2., 3., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the last one is less 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahh on the ipad I got all those exact same asnswers!!\n",
    "# so the backwards is convolve D with I but then padd the two 0s or however many we want on the left. Let's write a loop for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.tensor([1, 2, 4, 1, 1, 6], dtype=torch.float).view(1, 1, -1)\n",
    "K = torch.tensor([2, 3, 1], dtype=torch.float).view(1, 1, -1)\n",
    "O = torch.zeros(I.shape)\n",
    "for i in range(I.shape[2]):\n",
    "    if i > I.shape[2] - 3:\n",
    "        num_elem = I.shape[2] - i\n",
    "        O[0, 0, i] = I[0, 0, i:].dot(K[0, 0, :num_elem])\n",
    "    else:\n",
    "        O[0, 0, i] = I[0, 0, i : i + 3].dot(K[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[12., 17., 12., 11., 20., 12.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O  # perfect match!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[12., 17., 12., 11., 20., 12.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make it general and take in any size input in a function\n",
    "def oned_1channel_conv(I, K):\n",
    "    O = torch.zeros(I.shape)\n",
    "    # klen = K.shape[2]\n",
    "    for i in range(I.shape[2]):\n",
    "        if i > I.shape[2] - K.shape[2]:\n",
    "            num_elem = I.shape[2] - i\n",
    "            O[0, 0, i] = I[0, 0, i:].dot(K[0, 0, :num_elem])\n",
    "        else:\n",
    "            O[0, 0, i] = I[0, 0, i : i + K.shape[2]].dot(K[0, 0, :])\n",
    "    return O\n",
    "\n",
    "\n",
    "I = torch.tensor([1, 2, 4, 1, 1, 6], dtype=torch.float).view(1, 1, -1)\n",
    "K = torch.tensor([2, 3, 1], dtype=torch.float).view(1, 1, -1)\n",
    "O = oned_1channel_conv(I, K)\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the hard part is what if we have multiple input and output channels!!\n",
    "# first see what it should be\n",
    "\n",
    "I = torch.tensor([1, 2, 4, 1, 1, 6, 0, 0], dtype=torch.float).view(1, 1, -1)\n",
    "m = nn.Conv1d(1, 2, 3, bias=False, padding=0)\n",
    "m.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4687,  1.5251,  0.4510, -1.5939,  2.6247,  0.5719,  0.0000,\n",
       "            0.0000]]], grad_fn=<CopySlices>),\n",
       " tensor([[[0.6844, 1.9922, 2.0244, 0.2135, 2.2116, 2.7310, 0.0000, 0.0000]]],\n",
       "        grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now what it does is it has a separate 3x1 kernel for each of the 2 output channels. So it will have 2 3x1 kernels. So the output will be 2x6. Let's see if we can do this manually\n",
    "oned_1channel_conv(I, m.weight[0:1, :, :]), oned_1channel_conv(I, m.weight[1:2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4687,  1.5251,  0.4510, -1.5939,  2.6247,  0.5719],\n",
       "         [ 0.6844,  1.9922,  2.0244,  0.2135,  2.2116,  2.7310]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(I)  # again exactly what we expect, so can loop over output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the real hard part is if we have multiple input channels\n",
    "I = torch.tensor([1, 2, 4, 1, 1, 6, 0, 0], dtype=torch.float).view(1, 1, -1)\n",
    "# duplicate the input channel\n",
    "I = torch.cat((I, I), dim=1)\n",
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9274e-01, 9.2779e-01, 2.1114e+00, 1.8464e-03, 3.1480e-01, 8.7532e-01,\n",
       "         8.0751e-01, 4.8517e-02],\n",
       "        [1.3354e-02, 1.0712e+00, 1.9772e+00, 4.9204e-01, 1.3674e-01, 1.3449e-03,\n",
       "         7.4082e-01, 4.4926e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 8)\n",
    "a * a  # this is what the dot is, just multiplication then sum... so we can think of this as a zip and reduce, which is exxactly what we talked about in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or we just implement it ourself, so what we have to do is loop over the length of the input, and then for each element, we have to loop over the input channels and the output channels, and then do the dot product of the input channel and the kernel for that output channel\n",
    "\n",
    "\"\"\"\n",
    "Basics of the implementation\n",
    "We have thke primary outer loop be the input length, this corresponds to one element of the output along the entire out channel dimension. this is our parallelized one\n",
    "So we then move the weights and then can do the dot products have to do a lot of sums tho...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we made a basic function, let's test it\n",
    "import minitorch\n",
    "\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t2 = minitorch.tensor([[2, 3, 1]]).view(1, 1, 3)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[12.00 17.00 12.00 11.00 20.00 12.00]]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to work, now let's see if reverse works\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "# t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1]]).view(1, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "out.backward(minitorch.tensor([0, 1, 2, 3, 4, 5]).view(1, 1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [\n",
       " \t[\n",
       " \t\t[2.00 3.00 1.00]]\n",
       " \t[\n",
       " \t\t[2.00 3.00 1.00]]],\n",
       " (2, 1, 3))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yeah it seems to be working really well actually... huh I'll take it, so it works for 1 channel, the real test is if it works for multiple channels\n",
    "# try it for 2 output channels, but make their weights identical\n",
    "import minitorch\n",
    "\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t2 = minitorch.tensor([[2, 3, 1], [2, 3, 1]]).view(2, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "t2, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[1.00 2.00 4.00 1.00 1.00 6.00]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[12.00 17.00 12.00 11.00 20.00 12.00]\n",
       "\t\t[12.00 17.00 12.00 11.00 20.00 12.00]]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[0.00 1.00 2.00 3.00 4.00 5.00]\n",
       "\t\t[0.00 1.00 2.00 3.00 4.00 5.00]]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = minitorch.tensor([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]).view(1, 2, 6)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]\n",
       "\t[\n",
       "\t\t[60.00 39.00 21.00]]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try it again\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1], [2, 3, 1]]).view(2, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "d = minitorch.tensor([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]).view(1, 2, 6)\n",
    "out.backward(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]\n",
       "\t[\n",
       "\t\t[60.00 39.00 21.00]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[0.00 4.00 14.00 26.00 38.00 50.00]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[47., 33., 21.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "I = torch.tensor([1, 2, 4, 1, 1, 6, 0, 0], dtype=torch.float).view(1, 1, 8)\n",
    "I.requires_grad_(True)\n",
    "\n",
    "m = nn.Conv1d(1, 1, 3, bias=False, padding=0)\n",
    "m.weight.data = torch.tensor([[[2, 3, 1]]], dtype=torch.float)\n",
    "a = m(I)\n",
    "a.backward(torch.arange(6).view(1, 1, 6).float())\n",
    "m.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.,  7., 13., 19., 25., 19.,  5.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[47., 33., 21.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wait don't trust this I.grad, because those 0s at the end...\n",
    "I = torch.tensor(\n",
    "    [1, 2, 4, 1, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float\n",
    ").view(1, 1, -1)\n",
    "I.requires_grad_(True)\n",
    "\n",
    "m = nn.Conv1d(1, 1, 3, bias=False, padding=0)\n",
    "m.weight.data = torch.tensor([[[2, 3, 1]]], dtype=torch.float)\n",
    "a = m(I)\n",
    "a.backward(torch.arange(I.shape[2] - 2).view(1, 1, -1).float())\n",
    "m.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.,  7., 13., 19., 25., 31., 37., 43., 49., 55., 61., 67., 73.,\n",
       "          51., 13.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.grad  # we clearly see there's issues with this by adding a bunch of zeros, still changes the gradient because we arbitrarily set this thing, so let's manually calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[12., 17., 12., 11., 20., 12.]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.tensor([1, 2, 4, 1, 1, 6], dtype=torch.float).view(1, 1, -1)\n",
    "I.requires_grad_(True)\n",
    "K = torch.tensor([2, 3, 1], dtype=torch.float).view(1, 1, -1)\n",
    "K.requires_grad_(True)\n",
    "\n",
    "\n",
    "def oned_1channel_conv(I, K):\n",
    "    O = torch.zeros(I.shape)\n",
    "    # klen = K.shape[2]\n",
    "    for i in range(I.shape[2]):\n",
    "        if i > I.shape[2] - K.shape[2]:\n",
    "            num_elem = I.shape[2] - i\n",
    "            O[0, 0, i] = I[0, 0, i:].dot(K[0, 0, :num_elem])\n",
    "        else:\n",
    "            O[0, 0, i] = I[0, 0, i : i + K.shape[2]].dot(K[0, 0, :])\n",
    "    return O\n",
    "\n",
    "\n",
    "O = oned_1channel_conv(I, K)\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "O.backward(torch.arange(I.shape[2]).view(1, 1, -1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[47., 33., 21.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.,  7., 13., 19., 25.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[12.00 17.00 12.00 11.00 20.00 12.00]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see if minitorch gets it in the original way\n",
    "import minitorch\n",
    "\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t.requires_grad_(True)\n",
    "# t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1]]).view(1, 1, 3)\n",
    "# t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "# out.backward(minitorch.tensor([0,1,2,3,4,5]).view(1,1,6))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(minitorch.tensor([0, 1, 2, 3, 4, 5]).view(1, 1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[0.00 2.00 7.00 13.00 19.00 25.00]]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[12.00 17.00 12.00 11.00 20.00 12.00]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but if we try to get grad of both, it crashes? It worked this time, this seems weird and random... ok I guess??\n",
    "import minitorch\n",
    "\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1]]).view(1, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "# out.backward(minitorch.tensor([0,1,2,3,4,5]).view(1,1,6))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(minitorch.tensor([0, 1, 2, 3, 4, 5]).view(1, 1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[0.00 2.00 7.00 13.00 19.00 25.00]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it still fails if we have 2 channels\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1], [2, 3, 1]]).view(2, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "d = minitorch.tensor([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]).view(1, 2, 6)\n",
    "out.backward(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]\n",
       "\t[\n",
       "\t\t[60.00 39.00 21.00]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# made some changes, test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]\n",
       "\t[\n",
       "\t\t[60.00 39.00 21.00]]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we changed the order of operations, let's test it again\n",
    "import minitorch\n",
    "\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1], [2, 3, 1]]).view(2, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "d = minitorch.tensor([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]).view(1, 2, 6)\n",
    "out.backward(d)\n",
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[0.00 4.00 14.00 26.00 38.00 50.00]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad  # indeed this is correct, shows there's some other fundamental problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[12., 17., 12., 11., 20., 12.],\n",
       "         [12., 17., 12., 11., 20., 12.]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's compare to pytorch version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "I = torch.tensor([1, 2, 4, 1, 1, 6, 0, 0], dtype=torch.float).view(1, 1, 8)\n",
    "I.requires_grad_(True)\n",
    "m = nn.Conv1d(1, 2, 3, bias=False, padding=0)\n",
    "print(m.weight.shape)\n",
    "m.weight.data = torch.tensor([[[2, 3, 1], [2, 3, 1]]], dtype=torch.float).view(2, 1, 3)\n",
    "print(m.weight.shape)\n",
    "a = m(I)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.arange(6).view(1, 1, 6).float()\n",
    "# stack it\n",
    "d = torch.cat((d, d), dim=1)\n",
    "a.backward(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[47., 33., 21.]],\n",
       "\n",
       "        [[47., 33., 21.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  4., 14., 26., 38., 50., 38., 10.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lesliec/sarthak/mod4-Sarthak-Ti/.pixi/envs/default/lib/python3.12/site-packages/numba/core/typed_passes.py:336: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"minitorch/fast_conv.py\", line 36:\u001b[0m\n",
      "\u001b[1m\n",
      "\u001b[1mdef _tensor_conv1d(\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaPerformanceWarning(msg,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[\n",
       "\t[\n",
       "\t\t[47.00 33.00 21.00]]\n",
       "\t[\n",
       "\t\t[60.00 39.00 21.00]]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove prange and try again\n",
    "import minitorch\n",
    "\n",
    "t = minitorch.tensor([1, 2, 4, 1, 1, 6]).view(1, 1, 6)\n",
    "t.requires_grad_(True)\n",
    "t2 = minitorch.tensor([[2, 3, 1], [2, 3, 1]]).view(2, 1, 3)\n",
    "t2.requires_grad_(True)\n",
    "out = minitorch.Conv1dFun.apply(t, t2)\n",
    "d = minitorch.tensor([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]).view(1, 2, 6)\n",
    "out.backward(d)\n",
    "t2.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
